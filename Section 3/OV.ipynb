{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5234a4b6-93da-4f03-bc3b-24cc83498beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import open_clip\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model,_, preprocess =  open_clip.create_model_and_transforms(\"ViT-L-14\", pretrained='laion2b_s32b_b82k') #ViTB/32\n",
    "model = model.to(device)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "\n",
    "torch.set_num_threads(5)    \n",
    "torch.set_num_interop_threads(5)   \n",
    "\n",
    "class CustomWB(Dataset):\n",
    "    def __init__(self, root_dir, train, mask):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = 'train' if train else 'test'\n",
    "        self.mask = mask\n",
    "        self.transform = transforms.Compose(\n",
    "                [\n",
    "                 transforms.Resize(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                      std = [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "        self.image_dir = os.path.join(self.root_dir, self.split, 'images')\n",
    "        self.image_data = pd.read_csv(os.path.join(self.root_dir, self.split, 'info.csv'))\n",
    "        if self.mask:\n",
    "            assert \"mask\" in self.image_data.columns\n",
    "            self.mask_dir = os.path.join(self.root_dir, self.split, 'masks')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fp = self.image_data.iloc[idx]['fp']\n",
    "        label = torch.tensor(self.image_data.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, fp)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        if self.mask:\n",
    "            mask_path = os.path.join(self.mask_dir, self.image_data.iloc[idx]['mask'])\n",
    "            mask = torch.from_numpy(np.load(mask_path)).float()\n",
    "            attribute = torch.tensor(self.image_data.iloc[idx]['attribute'], dtype=torch.long)\n",
    "            return image, label, attribute\n",
    "        else:\n",
    "            return image, label\n",
    "\n",
    "\n",
    "testset = CustomWB(root_dir='../../../Dataset/data/waterbirds_variant/waterbirds_zeta_eg/', train=False, mask=True)\n",
    "dataloader = DataLoader(testset, batch_size=32, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4576f0-20d3-49b2-8a95-572649882f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.9920,  2.0092,  2.0263,  ...,  2.0777,  1.9920,  2.0263],\n",
       "          [ 1.9749,  1.9920,  1.9920,  ...,  2.2147,  2.1462,  2.1633],\n",
       "          [ 1.9578,  1.9749,  1.9920,  ...,  2.2318,  2.2147,  2.2318],\n",
       "          ...,\n",
       "          [-1.0904, -1.0219, -1.0904,  ...,  1.1358,  1.1700,  1.2214],\n",
       "          [-1.0904, -1.1589, -1.0562,  ...,  1.1187,  1.1358,  1.1872],\n",
       "          [-1.1075, -1.0733, -1.1247,  ...,  1.1187,  1.1187,  1.2214]],\n",
       " \n",
       "         [[ 1.9034,  1.9209,  1.9384,  ...,  2.0434,  1.8859,  1.8859],\n",
       "          [ 1.8859,  1.9034,  1.9209,  ...,  2.2360,  2.0959,  2.0609],\n",
       "          [ 1.8683,  1.8859,  1.9034,  ...,  2.2710,  2.2360,  2.2185],\n",
       "          ...,\n",
       "          [-0.7227, -0.6001, -0.6352,  ...,  0.6604,  0.6954,  0.7479],\n",
       "          [-0.6702, -0.7227, -0.5826,  ...,  0.6429,  0.6604,  0.7129],\n",
       "          [-0.6527, -0.6001, -0.6352,  ...,  0.6429,  0.6429,  0.7479]],\n",
       " \n",
       "         [[ 1.9603,  1.9777,  1.9951,  ...,  2.0300,  1.8208,  1.7511],\n",
       "          [ 1.9080,  1.9254,  1.9428,  ...,  2.2740,  2.0823,  1.9951],\n",
       "          [ 1.8731,  1.8905,  1.9080,  ...,  2.3786,  2.3088,  2.2391],\n",
       "          ...,\n",
       "          [-0.3055, -0.3055, -0.4624,  ...,  0.8797,  0.9145,  0.9668],\n",
       "          [-0.4450, -0.5495, -0.4973,  ...,  0.8622,  0.8797,  0.9319],\n",
       "          [-0.5321, -0.5495, -0.6193,  ...,  0.8622,  0.8622,  0.9668]]]),\n",
       " tensor(1),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a743a9-8a19-42c0-a464-c44564348f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████| 288/288 [02:32<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for label=0, sensitive=0: 0.91957 (total: 3680.0)\n",
      "Accuracy for label=0, sensitive=1: 0.34130 (total: 920.0)\n",
      "Accuracy for label=1, sensitive=0: 0.79674 (total: 920.0)\n",
      "Accuracy for label=1, sensitive=1: 0.98641 (total: 3680.0)\n",
      "Overall accuracy: 0.87620 (total: 9200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import deque, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_epoch(vlm,   dataloader):  \n",
    "    vlm = vlm.to(device)\n",
    "    vlm.eval()   \n",
    "    visual = vlm.visual\n",
    "\n",
    "    texts_label = [\"a photo of a landbird.\", \"a photo of a waterbird.\"] \n",
    "    text_label_tokened = tokenizer(texts_label).to(device)\n",
    "    text_embeddings = vlm.encode_text(text_label_tokened)#[:,:length,:]\n",
    "    text_embeddings = text_embeddings/text_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    overall_correct = 0\n",
    "    overall_total = 0\n",
    "    correct = defaultdict(float)\n",
    "    total = defaultdict(float)\n",
    "\n",
    "    for step, (test_input, test_target, sensitive_real) in enumerate(tqdm(dataloader, desc=\"Testing\")):\n",
    "        with torch.no_grad(): \n",
    "\n",
    "            test_input = test_input.to(device)\n",
    "            img_embeddings = vlm.encode_image(test_input).squeeze(1) \n",
    "\n",
    "            logits_per_image = torch.mm(img_embeddings, text_embeddings.t())\n",
    "\n",
    "            probs = logits_per_image.softmax(dim=1)\n",
    "            _, predic = torch.max(probs.data, 1)\n",
    "            predic = predic.detach().cpu()\n",
    "\n",
    "            label = test_target.detach().cpu()\n",
    "            overall_correct += (predic == label).sum()\n",
    "            overall_total += len(test_target.reshape(-1).detach().cpu())\n",
    "            \n",
    "            unique_groups = np.unique(np.stack([label, sensitive_real], axis=1), axis=0)\n",
    "            for group in unique_groups:\n",
    "                mask = (label == group[0]) & (sensitive_real == group[1])\n",
    "                correct[tuple(group)] += (predic[mask] == label[mask]).sum()\n",
    "                total[tuple(group)] += mask.sum()\n",
    "    \n",
    "\n",
    "    for group, correct_count in correct.items():\n",
    "        accuracy = correct_count / total[group]\n",
    "        print(f'Accuracy for label={group[0]}, sensitive={group[1]}: {accuracy:.5f} (total: {total[group]})')\n",
    "\n",
    "    overall_accuracy = overall_correct / overall_total\n",
    "    print(f'Overall accuracy: {overall_accuracy:.5f} (total: {overall_total})')\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "test_epoch(model, dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DLcourse)",
   "language": "python",
   "name": "dlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
